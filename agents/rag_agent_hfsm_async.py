"""
Async RAG Agent HFSM
====================

Async version of RAG Agent using HFSM architecture.
"""

import sys
import os
import logging
from datetime import datetime

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from finitestatemachineAgent.hfsm_agent_async import AsyncAgentEngine
from providers.llm_client_async import AsyncLLMClient
from core.executor_async import AsyncToolExecutor
from core.context_async import AsyncExecutionContext, SafetyMonitor, SafetyLimitExceeded
from core.registry import ToolRegistry
import tools.rag_tools as rag_tools
from typing import AsyncIterator
from finitestatemachineAgent.hfsm_agent_async import Transition

logger = logging.getLogger(__name__)


class AsyncRAGAgentFSM:
    """
    Async RAG Agent using Hierarchical Finite State Machine.
    
    Uses async/await for better concurrency and performance.
    """
    
    def __init__(
        self,
        embedding_manager,
        model: str = "google/gemini-2.0-flash-exp:free",
        skip_validation: bool = False,
        max_global_requests: int = 50
    ):
        # Initialize RAG tools
        rag_tools.initialize_rag_tools(embedding_manager)
        
        # Setup registry
        registry = ToolRegistry()
        
        # Register all RAG tools
        tools_list = [
            rag_tools.search_documents,
            rag_tools.get_stock_price,
            rag_tools.compare_stocks
        ]
        
        for tool_func in tools_list:
            if hasattr(tool_func, '_tool_name'):
                registry.register(
                    name=tool_func._tool_name,
                    description=tool_func._tool_description,
                    function=tool_func,
                    args_model=tool_func._args_model
                )
        
        # Create async executor and LLM
        executor = AsyncToolExecutor(registry)
        llm = AsyncLLMClient(model=model)
        
        # Get current date for temporal context
        from datetime import datetime
        current_date = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # System instruction
        system_instruction = f"""
Voc√™ √© o Finance.AI, um assistente financeiro especialista.

DATA/HORA ATUAL: {current_date}

REGRAS CRITICAS:
1. Para conceitos econ√¥micos, defini√ß√µes e contexto (ex: Selic, Copom, Infla√ß√£o, PIB), SEMPRE use 'search_documents'.
2. Para cota√ß√µes e performance de ativos (ex: PETR4, NVDA, compara√ß√µes), SEMPRE use 'get_stock_price' ou 'compare_stocks'.

REGRA ANTI-REDUND√ÇNCIA (CR√çTICO):
4. ANTES de chamar qualquer ferramenta, VERIFIQUE se voc√™ j√° tem os dados necess√°rios nas chamadas de ferramentas anteriores (tool calls).
5. Se voc√™ j√° chamou uma ferramenta e recebeu os dados, N√ÉO chame a mesma ferramenta novamente com os mesmos par√¢metros.
6. Use os resultados das ferramentas j√° executadas para responder a pergunta. S√≥ chame uma nova ferramenta se realmente precisar de informa√ß√µes adicionais diferentes.

REGRA TEMPORAL (CR√çTICO):
7. Use a DATA/HORA ATUAL fornecida acima para contexto temporal
8. N√ÉO invente datas - use apenas informa√ß√µes dos dados retornados pelas ferramentas
9. Quando mencionar per√≠odos (1 m√™s, 6 meses, 1 ano), calcule a partir da data atual

Para perguntas conceituais sobre finan√ßas, economia ou mercado financeiro, priorize sempre o uso de 'search_documents'.
Nunca responda diretamente sem utilizar as ferramentas de busca disponiveis.
"""
        
        # Custom validation function for RAG tools
        async def rag_validation(context, tool_name, result):
            """
            Custom validation logic for RAG agent tools.
            Returns True if the tool result is valid, False otherwise.
            """
            if tool_name in ("get_stock_price", "compare_stocks"):
                # For stock tools, check if result has success=True
                return isinstance(result, dict) and result.get("success") == True
            
            elif tool_name == "search_documents":
                # For document search, check if we have results
                return isinstance(result, dict) and result.get("results") and len(result.get("results", [])) > 0
            
            # Default: accept any non-None result
            return result is not None
        
        # Custom planning prompt enhancer for parallel execution
        def enhance_rag_planning_prompt(default_prompt, context):
            """
            Enhances default planning prompt with RAG-specific divide-and-conquer strategy.
            Instructs LLM to break complex financial queries into smaller independent tasks.
            """
            # Add RAG-specific planning instructions
            enhancement = """

ESTRAT√âGIA DIVIDIR E CONQUISTAR (RAG AGENT):

Para consultas financeiras complexas, voc√™ DEVE quebrar em sub-tarefas independentes:

1. **Compara√ß√µes de Ativos**: 
   - Se comparar m√∫ltiplos ativos (ex: "Compare PETR4, VALE3 e ITUB4")
   - Crie um branch para cada ativo
   - Cada branch pesquisa um ativo espec√≠fico

2. **An√°lises Multi-T√≥pico**:
   - Se a pergunta envolve m√∫ltiplos conceitos (ex: "Explique Selic, Copom e infla√ß√£o")
   - Crie um branch para cada conceito
   - Cada branch pesquisa um conceito espec√≠fico

3. **Consultas Compostas**:
   - Se combina dados + conceitos (ex: "Qual o pre√ßo do PETR4 e o que √© dividend yield?")
   - Branch 1: Buscar pre√ßo do ativo
   - Branch 2: Buscar conceito te√≥rico

REGRAS IMPORTANTES:
- S√≥ paralelizar se as sub-tarefas forem INDEPENDENTES
- Cada branch deve ter um objetivo claro e espec√≠fico
- M√°ximo de 3 branches.
- Para consultas simples (1 ativo, 1 conceito), use strategy: "single"

EXEMPLOS:

Query: "Compare NVDA e TSLA"
‚Üí strategy: "parallel_research"
‚Üí branches: [
    {"id": "nvda", "goal": "Pesquisar pre√ßo e dados da NVDA"},
    {"id": "tsla", "goal": "Pesquisar pre√ßo e dados da TSLA"}
]

Query: "Qual o pre√ßo do PETR4?"
‚Üí strategy: "single" (consulta simples, n√£o precisa paralelizar)

Query: "Explique Selic, Copom e CDI"
‚Üí strategy: "parallel_research"
‚Üí branches: [
    {"id": "selic", "goal": "Pesquisar conceito de Selic"},
    {"id": "copom", "goal": "Pesquisar conceito de Copom"},
    {"id": "cdi", "goal": "Pesquisar conceito de CDI"}
]"""
            
            return default_prompt + enhancement
        
        # Custom post-router hook to enforce tool usage
        async def enforce_tool_usage(context, transition):
            """
            RAG-specific hook: Reject direct answers, force tool usage.
            
            This keeps the engine domain-agnostic while allowing
            RAG agent to enforce its own rules.
            """

            
            if transition.to == "AnswerState" and transition.reason == "Direct answer generation":
                # LLM tried to answer directly without tools - unacceptable for RAG
                retry_count = await context.get_memory("rag_tool_retry", 0)
                
                if retry_count < 2:
                    await context.set_memory("rag_tool_retry", retry_count + 1)
                    logger.info(f"üîÑ [RAG] Forcing tool usage (attempt {retry_count + 1}/2)")
                    
                    # Override transition to retry
                    return Transition(to="RetryState", reason="RAG agent requires tool usage")
                else:
                    logger.error("‚ùå [RAG] LLM refusing to use tools after retries")
                    # Let it fail to RetryState
                    return Transition(to="RetryState", reason="Tool usage required")
            
            # Reset retry count on successful tool usage
            if transition.to == "ToolState":
                await context.set_memory("rag_tool_retry", 0)
        
        # Create async agent engine with custom validation and parallel execution
        self.agent = AsyncAgentEngine(
            llm=llm,
            registry=registry,
            executor=executor,
            system_instruction=system_instruction,
            tool_choice=None,
            skip_validation=skip_validation,
            validation_fn=rag_validation,  # Custom validation
            
            # Enable parallel execution with custom planning
            enable_parallel_planning=True,
            planning_system_prompt=enhance_rag_planning_prompt,  # Incremental enhancement
            # merge_fn=None -> uses default append merge
            max_parallel_branches=3,    # üî• Limit width to 3 branches per fork
            max_global_requests=max_global_requests,  # Safety limit
            # üî• Enable built-in intent analysis
            enable_intent_analysis=True,
            intent_analysis_llm=llm,
            
            # üî• Custom Redirect Prompt
            redirect_system_prompt=f"""Voc√™ √© o **Finance.AI**, um assistente especializado em Mercado Financeiro e Economia.
Data de hoje: {datetime.now().strftime('%d/%m/%Y')}

DIRETRIZES DE RESPOSTA R√ÅPIDA:
**Suas capacidades:**
- üìä Cota√ß√µes de a√ß√µes em tempo real (get_stock_price)
- üìà Compara√ß√£o de performance entre a√ß√µes (compare_stocks)
- üìö Busca em documentos sobre conceitos econ√¥micos (search_documents)

**Instru√ß√µes de resposta:**
- Se for SAUDA√á√ÉO: Cumprimente de forma amig√°vel e ofere√ßa ajuda
- Se for PERGUNTA SOBRE CAPACIDADES: Explique brevemente o que voc√™ pode fazer com exemplos
- Se for FORA DO ESCOPO: Explique educadamente que voc√™ √© especializado em finan√ßas e sugira temas v√°lidos

**Limita√ß√µes:**
- N√£o d√° recomenda√ß√µes de investimento
- N√£o prev√™ pre√ßos futuros
- Foco exclusivo em finan√ßas e economia
- Nunca responda perguntas fora do escopo de finan√ßas e economia
- Caso o usu√°rio tente fazer perguntas fora do escopo, responda educadamente explicando que voc√™ √© especializado em finan√ßas, explique suas capacidades e sugira temas v√°lidos"""
        )
        
        logger.info("‚úÖ [RAG] Built-in intent analysis enabled")
    
    async def run_stream(
        self,
        query: str,
        chat_history=None,
        enable_streaming: bool = True  # - Control streaming
    ) -> AsyncIterator[str]:
        """
        Run agent with async streaming.
        
        Args:
            query: User query
            chat_history: Optional chat history
            enable_streaming: If True, stream response. If False, generate complete response first.
            
        Yields:
            Response tokens as they arrive
        """
        # üî• DEBUG: Entry point
        logger.info("=" * 80)
        logger.info("üöÄ [RAG] run_stream() CALLED")
        logger.info(f"üìù [RAG] Query: {query[:100]}...")
        logger.info("=" * 80)
        
        # Run agent and get context
        
        # Create context manually with Safety Monitor
        monitor = SafetyMonitor(max_requests=self.agent.max_global_requests)
        context = AsyncExecutionContext(user_query=query, safety_monitor=monitor)
        
        await context.set_memory("system_instruction", self.agent.system_instruction)
        await context.set_memory("chat_history", chat_history or [])
        await context.set_memory("enable_streaming", enable_streaming)  # üî• Set streaming flag
        
        try:
            # üî• IntentAnalysis is now the initial state in dispatch
            # No need to call it manually here
            
            # Run dispatch (starts from IntentAnalysisState)
            await self.agent.dispatch(context)
            
            # Store context for later access
            self.context = context
            
            # Collect answer for finalization
            answer = []
            
            # Stream from context memory (not from state instance)
            stream = await context.get_memory("answer_stream")
            if stream:
                async for token in stream:
                    answer.append(token)
                    yield token
            
            # Finalize response with metadata
            final_answer = "".join(answer)
            await self._finalize_response(final_answer, context)

        except SafetyLimitExceeded as e:
            yield f"\n\nüõë **SYSTEM HALT**: {str(e)}"
            # Optionally log this event
            # logger.error(f"Circuit breaker tripped: {e}")
            return
    
    async def _finalize_response(
        self,
        content: str,
        context
    ):
        """
        Calculate metrics and store in context memory
        """
        sources_used = []
        scores = []
        has_stock_data = False
        
        # include merged tool calls from parallel execution
        merged_tools = await context.get_memory("merged_tool_calls", [])
        all_calls = (context.tool_calls or []) + merged_tools
        
        for call in all_calls:
            tool_name = call.get("tool_name")
            result = call.get("result", {})
            
            if tool_name == "search_documents" and isinstance(result, dict):
                for doc in result.get("results", []):
                    meta = doc.get("metadata", {})
                    src = meta.get("source")
                    if src and src not in sources_used:
                        sources_used.append(src)
                    if "score" in doc:
                        scores.append(doc["score"])
            
            elif tool_name in ("get_stock_price", "compare_stocks"):
                if isinstance(result, dict) and result.get("success"):
                    sources_used.append(f"yfinance:{tool_name}")
                    has_stock_data = True
        
        confidence = self._calculate_confidence(
            has_stock_data,
            scores,
            sources_used
        )
        
        # Store in context.memory so API can access it
        await context.set_memory("sources_used", sources_used)
        await context.set_memory("confidence", confidence)
        await context.set_memory("final_answer", content)
        
        # - Store total requests
        if hasattr(context, 'safety_monitor'):
            await context.set_memory("total_requests", context.safety_monitor.count)
    
    def _calculate_confidence(
        self,
        has_stock_data: bool,
        scores: list,
        sources_used: list
    ) -> str:
        if has_stock_data:
            return "high"
        
        if scores:
            avg_score = sum(scores) / len(scores)
            max_score = max(scores)
            
            if max_score > 0.7 or (avg_score > 0.6 and len(scores) >= 2):
                return "high"
            if avg_score >= 0.5:
                if max_score > 0.6:
                    return "medium"
                return "low"
            return "low"
        
        return "low" if not sources_used else "medium"
